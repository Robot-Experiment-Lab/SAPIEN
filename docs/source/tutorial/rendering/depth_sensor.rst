.. _depth_sensor:


SAPIEN Realistic Depth
======================

.. highlight:: python

In this tutorial, you will learn the following:

* Simulating realistic depth with ``StereoDepthSensor``

The full script can be downloaded from :download:`rt_stereodepth.py <../../../../examples/rendering/rt_stereodepth.py>`

.. note::
   You are required to have an NVIDIA GPU with 6.0 <= compute capability <= 8.6 to use ``StereoDepthSensor``. For users without available NVIDIA
   GPU, there is an alternative implementation named ``ActiveLightSensor`` that can run on CPU. Note that ``ActiveLightSensor`` will produce slightly
   different results compared to ``StereoDepthSensor`` and will be much slower. Usage of ``StereoDepthSensor`` is always recommended when an NVIDIA
   GPU is available.


Sim-to-real gap
---------------
Simulation is a powerful tool for developing and testing computer vision/robotic algorithms, but there is often a significant gap between the performance
of the algorithms in simulation and its performance in the real world. This is known as the **sim-to-real gap**. Among different types of input of real-world
oriented algorithms, depth is one of the most important type of data that has extensive use. It is found that using depth data from simulated depth sensor as
input can improve sim-to-real performance by a large margin compared to using ground truth depth [1]. Inspired by this, SAPIEN is integrated with a **real-time**
active stereo sensor simulation pipeline that will produce depth similar to real-world depth sensors. This tutorial will guide you through the process of using
``StereoDepthSensor`` to generate *SAPIEN Realistic Depth* and apply it to your system.


Realistic depth sensor simulation
---------------------------------
Let's see an example of generating SAPIEN Realistic Depth with ``StereoDepthSensor``. First of all, we need to import the class we will be using, and set
up our renderer and scene:

::

    from sapien.core import Pose
    from sapien.sensor import StereoDepthSensor, StereoDepthSensorConfig

.. literalinclude:: ../../../../examples/rendering/rt_stereodepth.py
   :dedent: 0
   :lines: 77-86

We will be using ray-tracing renderer in this section. While ``StereoDepthSensor`` also supports rasterization renderer, using ray-tracing renderer can
give us more realistic results. The scene we will be using is a simulated scene aligned to the following real scene:

.. figure:: assets/aligned_rgb_real.png
    :width: 720px
    :align: center

You can find the detailed code of ``build_scene`` in the full script.

Now, let's add a ``StereoDepthSensor`` to the scene:

::

    sensor_config = StereoDepthSensorConfig()
    sensor = StereoDepthSensor('sensor', scene, sensor_config)

``StereoDepthSensor`` supports a large number of configurable parameters to maximize its flexibility of simulating different sensors. For more information,
Please check the API doc for class ``StereoDepthSensorConfig``. ``sensor`` behaves very similar to a mounted camera. You can ``set_pose``, ``take_picture``,
and what's more, ``compute_depth`` on the sensor:

.. literalinclude:: ../../../../examples/rendering/rt_stereodepth.py
    :dedent: 0
    :lines: 90-95

One important differences between mounted camera and ``sensor`` is that while mounted camera will only take picture of an RGB image, ``sensor`` will take
another pair of infrared images, which will be used to compute depth. After calling ``take_picture``, the RGB image and infrared images will be saved within
``sensor``. We can take a look at them by calling ``sensor.get_rgb`` and ``sensor.get_ir``:

.. figure:: assets/aligned_rgb.png
    :width: 720px
    :align: center

    Simulated RGB image from ``sensor``.

.. figure:: assets/aligned_ir.png
    :width: 720px
    :align: center

    One of the simulated infrared images from ``sensor``.

Calling ``compute_depth`` after ``take_picture`` will generate the depth map and save it within the ``sensor``. Similarly, we can get the computed depth by calling
``sensor.get_depth``. Let's compare our simulated depth with depth generated by real-world depth sensor:

.. figure:: assets/aligned_depth_real.png
    :width: 720px
    :align: center

    Depth map generated by real-world depth sensor.

.. figure:: assets/aligned_depth.png
    :width: 720px
    :align: center

    SAPIEN Realistic Depth generated by ``sensor``.

Note how SAPIEN Realitic Depth is able to reproduce the error pattern of the real sensor.


Boosting simulation speed
-------------------------
The entire depth simulation pipeline in SAPIEN has been highly-optimized in GPU. It is possible to achieve real-time (60+ FPS) performance with SAPIEN under certain
resolution. Let's see a few tricks that can further boost the speed of ``StereoDepthSensor``.

If depth is the only needed data and RGB data is not needed, you can specify ``infrared_only`` when calling ``take_picture``:

::

    sensor.take_picture(infrared_only=True)

This can save the time for rendering RGB image.

``StereoDepthSensor`` also supports resampling the generated depth map, so that you don't need to attach a slow CPU function to it. By default, the original depth map
will be in the same resolution as the infrared resolution. It will then be transformed into the frame of the RGB camera, with the new size being the same as RGB resolution.
Using this feature you can freely realize downsampling/upsampling without additional overhead.

In addition, ``StereoDepthSensor`` has a function ``get_depth_dl_tensor`` that can return a DLPack tensor. Through DLPack, the output depth map can be seamlessly
transferred into gpu tensors of common deep learning library such as PyTorch and TensorFlow. This can save the time for unnessecary GPU-to-CPU transfer. For example,
directly creating a PyTorch GPU tensor from ``StereoDepthSensor`` is as simple as one line:

::

    tensor = torch.utils.dlpack.from_dlpack(sensor.get_depth_dl_tensor()).clone()

In general, lowering ``rt_samples_per_pixel`` for renderer, ``ir_resolution`` of sensor config and ``max_disp`` of sensor config are all good ways to enhance  computation
speed. However, note that these changes might also have effect on the output depth map. You can freely adjust the parameters until you find the settings that satisfies
your need.


References
----------
[1] Xiaoshuai Zhang, Rui Chen, Ang Li, Fanbo Xiang, Yuzhe Qin, Jiayuan Gu, Zhan Ling, Minghua Liu, Peiyu Zeng, Songfang Han, Zhiao Huang, Tongzhou Mu, Jing Xu, Hao Su, "Close the Optical Sensing Domain Gap by Physics-Grounded Active Stereo Sensor Simulation," in IEEE Transactions on Robotics (T-RO).
